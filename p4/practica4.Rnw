\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}


\title{ Práctica 4 \\
\begin{large}
     Fundamentos de la Ciencia de Datos
\end{large}
}
\author{Samuel Aós Paumard,\\
Enrique Coronado Barco,\\
Carmen Martínez Estévez,\\
Alberto Martínez Ortega}
\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

\section{Ejericio 1}
\textbf{La primera parte consiste en la realización de un ejercicio en clase con ayuda del profesor en el que se va a realizar un análisis de clasificación no supervisada de Datos con R aplicando todos los conceptos vistos en el tema. En dicho ejercicio se resolverá, utilizando el algoritmo K-means, el mismo problema que el visto en la descripción teórica del tema. Es decir, para la misma muestra1 que se utilizó para hacer de forma manual el primer ejercicio de clasificación no supervisada, se deberán obtener los centroides de los clusters obtenidos.}

Mediante la muestra de calificaciones vista en teoría, compuesta por las calificaciones de Teoría y Laboratorio, realizamos una matriz en cuya transpuesta observamos los valores de la primera columna como los correspondientes a Teoría y la segunda los de Laboratorio.

<<cargaMatrizCalificaciones>>=
m<-matrix(c(4,4,3,5,1,2,5,5,0,1,2,2,4,5,2,1),2,8)
(m<-t(m))
@

Cargamos en la variable \textit{centroides} los valores de los centroides C1=[0,1] y C2=[2,2].
<<cargaCentroides>>=
centroides<-matrix(c(0,1,2,2),2,2)
(centroides<-t(centroides))
@

Realizamos la clasificación k-means usando ambas variables generadas anteriormente.
<<kmeans>>=
(clasificacion<-kmeans(m,centroides,4))
@

Como podemos observar obtenemos un conjunto amplio de valores entre los que cabe destacar los valores de los centroides del cluster obtenido C1=[1.25,1.50] y C2=[4.00,4.75]. Otro valor importante obtenido es el vector cluster, el cual a continuación introduciremos como primera columna de nuestro conjunto de clasificaciones.
<<unificacion_cluster>>=
(m=cbind(clasificacion$cluster,m))
@

Extraemos todos los valores del conjunto de datos los cuales en la primera columna, la indicativa de los valores del cluster, tengan valor 1.
<<extraccion_c1>>=
(mc1=subset(m,m[ ,1]==1)) 
@

De igual forma lo hacemos para aquellos valores que correspondan con el cluster de valor 2.
<<extraccion_c2>>=
(mc2=subset(m,m[ ,1]==2))
@

Una vez clasificados los datos de acuerdo a sus clusteres, extraemos la columna referente al valor de estos para facilitar la lectura.

<<formateo>>=
(mc1=mc1[,-1])
(mc2=mc2[,-1])
@

\newpage
\section{Ejericio 2}
\textbf{La segunda parte consiste en el Desarrollo por parte de cada alumno del enunciado y la solución de un ejercicio en el que se realice un análisis con R de clasificación no supervisada introduciendo modificaciones sobre el ejercicio hecho en clase.}

En cuanto a las librerías usadas para esta parte, encontramos:
\begin{itemize}
     \item tidyverse: conjunto de paquetes de R especialmente desarollados para Ciencia de Datos y una mayor correlación entre ellos con sintaxis común, por ejemplo.
     \item cluster: métodos para análisis de clusters de datos. Más extendido que su predecesor de 1990 "Finding Groups in Data".
     \item factoextra: extrae y muestra los datos de manera sencilla. También tiene métodos para simplificar algunos análisis de clusters.
     \item gridExtra: permite agrupar múltiples gráficas en una sola o dibujar tablas.
     \item magick: procesamiento de imagen de una forma sencilla sin perder eficacia. Necesario para LearnClust.
     \item LearnClust: para el ejercicio 3, será explicado convenientemente más adelante.
\end{itemize}

<<librerias2, results=hide>>=
install.packages("LearnClust")
install.packages("factoextra")
install.packages("xlsx")
install.packages("magick")
library(tidyverse)
library(cluster)
library(factoextra)
library(gridExtra)
library(magick)
library(LearnClust)
@

En este caso, la muestra utilizada es otra de calificaciones modificada a partir de la anterior. Encontramos cinco columnas esta vez, frente a las dos anteriores. Esto es porque además de las notas de teoría y laboratorio se han añadido las calificaciones medias de prácticas, ensayos y entregas.
Si los cargamos desde el .txt donde están y eliminamos los valores NA, en caso de haberlos, obtenemos la siguiente matriz de datos:

<<cargadatos2>>=
calificaciones<-read.table("calificaciones.txt")
calificaciones<-na.omit(calificaciones)
calificaciones
@

Como no queremos que el algoritmo de agrupamiento dependa solo de una variable, estandarizamos los datos con scale, obteniendo la siguiente matriz de datos:

<<standard2>>=
calificaciones<-scale(calificaciones)
calificaciones
@

A continuación calculamos una matriz de distancia entre las filas, que será usada posteriormente para la visualización de esta según colores, dependiendo de si la distancia es 'baja', 'media' o 'alta'.

<<fig=true>>=
distancia<-get_dist(calificaciones)
fviz_dist(distancia, gradient = list(low = "#FDFEFF", mid = "#84B5E6", high = "#020D6C"))
@

Estudiamos los distintos casos con el algoritmo kmeans. Como parámetros a esta función encontramos los siguientes. 
\begin{itemize}
     \item calificaciones: matriz estandarizada con los datos a estudiar.
     \item centers={1,2,3,4}: al ser un número, significa el número de filas (distintas) que se van a escoger como centroides iniciales.
     \item nstart: si centers es un número, que lo es, hace referencia a cuántos sets aleatorios han de ser escogidos para llevar a cabo el método.
\end{itemize}

<<km2>>=
k2 <- kmeans(calificaciones, centers = 1, nstart = 25)
k3 <- kmeans(calificaciones, centers = 2, nstart = 25)
k4 <- kmeans(calificaciones, centers = 3, nstart = 25)
k5 <- kmeans(calificaciones, centers = 4, nstart = 25)
@

Una vez obtenidos los resultados de los 4 kmeans realizados, procedemos a mostrarlos en una gráfica a la que le pasamos la variable que guarda los resultados de cada uno, las calificaciones sobre las que se han obtenido dichos resultados y agregamos un título a cada una para tenerlas identificadas.
Esto es así porque, para mostrar los datos de una manera más ordenada y cuya comparación resulte más fácil, se han agrupado las 4 gráficas en una sola.

<<graficas2, fig=true>>=
p1<-fviz_cluster(k2, data = calificaciones) + ggtitle("K = 1")
p2<-fviz_cluster(k3, data = calificaciones) + ggtitle("K = 2")
p3<-fviz_cluster(k4, data = calificaciones) + ggtitle("K = 3")
p4<-fviz_cluster(k5, data = calificaciones) + ggtitle("K = 4")

grid.arrange(p1, p2, p3,p4, nrow = 2)
@
\\

Si observamos los resultados obtenemos las siguientes conclusiones:
Con k=1, es decir, un único centroide, todos los datos están agrupados bajo la misma clase, incluso los más alejados del centro, pues no hay más puntos de referencia.
Con k=2, es decir, dos centroides, vemos como Sonia y Jaime, que eran los más alejados ya pertenecen a otra clase distinta que el resto de los datos, que siguen pertenenciendo al primer centroide.
Con k=3, ocurre una cosa curiosa y es que Sonia y Jaime están tan alejados del resto de los datos y entre sí, que la clasificación hace que se separen en 2 clases distintas, dejando el resto de los datos sin modificar.
Con k=4, Sonia y Jaime siguen estando como antes. La diferencia se aprecia en los datos que pertenecían a la clase 'roja', que ha quedado dividida en dos clases diferentes, ajustando los centroides.

\newpage
\section{Ejericio 3}
\textbf{Aplicar y analizar el paquete de R: LearnClust: Learning Hierarchical Clustering Algorithms}

Se trata de un paquete realizado por un alumno de la UAH donde se nos permite conocer a fondo los entresijos de la clusterización jerárquica, aglomerativa y divisiva.
Cada función ofrece una opción ".details" que nos detalla, paso a paso, cómo se va iterando y calculando el resultado final. Además, permite el uso de funciones distancia, inicialización de datos y otras funciones de interés que favorecen un estudio profundo de estos algoritmos, su recodificación con posibles mejoras, así como una realización del propio algoritmo de manera más detallada y parcial, según interese al usuario.
El paquete contiene 3 funciones para realizar clustering, cada una con su .details. Hemos realizado un análisis de los resultados con cada uno de los 3 tipos: aglomerativeHC, divisiveHC, correlationHC.
Las 2 primeras funciones aceptan los 3 mismos parámetros:
\begin{itemize}
     \item data: matriz, vector o dataframe numérico. Será transformado a matriz y lista
     \item distance: tipo de distancia usado. Existen diversos tipos:
      \begin{itemize}
          \item CAN: Distancia Canberra
          \item CHE: Distancia Chebyshev
          \item EUC: Distancia Euclídea 
          \item MAN: Distancia Manhattan 
          \item OCT: Distancia Octil
      \end{itemize}
     \item approach: una vez calculada la distancia, tipo de acercamiento usado. Existen diversos tipos:
      \begin{itemize}
          \item MAX: Máximo del vector
          \item MIN: Mínimo del vector
          \item AVG: Media del vector
      \end{itemize}
\end{itemize}

Definimos la matriz a utilizar y la convertimos a dataframe.

<<datos3>>=
matrizA<-c(4,4,3,5,1,2,5,5,0,1,2,2,4,5,2,1)
matriz<-matrix(matrizA,ncol=2)
matriz<-data.frame(matriz)
matriz
@

Comenzamos con el aglomerativeHC, distancia: euclídea, aproach: máximo.
Según la documentación del método, los datos son formateados y se crean los clusters, calculando la matriz de distancias con los parámetros seleccionados. Se escoge la distancia y los clusters. Estos se agrupan en uno nuevo y actualiza la lista de clusters. Se itera hasta que solo existe un único cluster.
Los resultados son los siguientes:
<<aglomerativo>>=
agglomerativeHC(matriz, 'EUC', 'MAX')
@

\newpage

Si ejecutamos la version .details podemos observar una cantidad abrumadora de explicaciones sobre todos los cálculos realizados a lo largo de la ejecución del algoritmo. Por supuesto, al final se nos muestra el resultado, que es idéntico al obtenido anteriormente.

<<agdetails>>=
agglomerativeHC.details(matriz, 'EUC', 'MAX')
@

Al correr el comando 'Sweave("practica4.Rnw")', se nos ofrece una descripción detallada de los pasos del algoritmo que no hemos conseguido traspasar al .tex para mostrarlo en el pdf.
Como correr otra vez las versiones .details nos dejaría un pdf demasiado largo, hemos omitido esta comprobación en el resto.

Vamos ahora con el divisiveHC, distancia: chebyshev, approach: mínimo.
Volviendo a referirnos al manual, el algoritmo trata lo siguiente: primero se formatean los datos y se crea un cluster que incluye cada uno de los elementos. Posteriormente inicializa otros clusters usando los elementos iniciales. Se calcula una matriz de distancias con los clusters creados en el tercer paso y se escoge la distancia maxima para dividir los clusters. Se dividen en dos clusters complementarios cada uno y se actualiza la lista de clusters. Esto se hace en repetidas ocasiones hasta que cada cluster no pueda ser dividido otra vez. El resultado final es una lista con todos los clusters simples. Los resultados obtenidos son los siguientes:

<<divisivo, fig=true>>=
divisiveHC(matrizA,'CHE','MIN')
@

Por último, vamos a hacer referencia la método correlationHC. Se diferencia de los dos anteriores en que admite una serie de parámetros más larga que los anteriores. Si bien comparte los anteriores, añade otros como un target(NULL por defecto), unos pesos a considerar en cada paso, si queremos o no normalizar los pesos y una opción para obtener una salida gráfica.
Obtenemos los siguientes resultados para el target y el peso escogidos.
<<corre3>>=
target<- c(2,3)
weight<- c(1,5)
correlationHC(matriz,target,weight)
@

Para terminar, nos gustaría dejar claro que consideramos al LearnClust un paquete muy completo. Permite un aprendizaje en profundidad sobre clustering, quizá en algunos momentos con algo de sobreinformación, pero mejor eso que no tener ni idea de lo que sucede en los entresijos de los algoritmos.
Nos gusta especialmente porque permite hacer cálculos sueltos sobre distancias, incluyendo muchos tipos para calcularlas. También destacamos el hecho de que existan multitud de métodos para poder modificar de manera exahustiva el comportamiento de los algoritmos, así como el poder recodificarlos, modificarlos, ampliarlos o incluso reducirlos a conveniencia. 
Permite una libertad total para la experimentación con este tipo de problemas. De hecho, se podría usar para realizar cáclulos complejos, tediosos a mano, que no tuvieran nada que ver con el clustering. Sirve también para el desarrollo de problemas de clase si queremos comparar algún resultado completo o parte de un procedimiento y no tener que hacerlo todo entero de manera opaca.

\end{document}
